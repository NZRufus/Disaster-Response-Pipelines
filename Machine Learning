# ML Pipeline Preparation
Follow the instructions below to help you create your ML pipeline.
### 1. Import libraries and load data from database.
- Import Python libraries
- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)
- Define feature and target variables X and Y

# import libraries
from sqlalchemy import create_engine
import numpy as np
import pandas as pd

from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report,confusion_matrix, precision_score,\
recall_score,accuracy_score,  f1_score,  make_scorer
from sklearn.base import BaseEstimator, TransformerMixin
import nltk
import re
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
from nltk import word_tokenize

import pickle


engine = create_engine('sqlite:///DisasterResponse.db')
df = pd.read_sql("SELECT * from messages", engine)
df=df.drop(['original'], axis=1)
df2 =df.dropna()
X = df2['message']
col_names=df2.drop(['id','message','genre'], axis=1).columns
Y =df2[col_names]

Y.shape

Y.isnull().sum()

X.shape

### 2. Write a tokenization function to process your text data

url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

def replace_urls(text):
    """
    Outputs editted version of the input Python str object `text` 
    replacing all urls in text with str 'urlplaceholder'.
    
    INPUT:
        - text - Python str object - a raw text data
        
    OUTPUT:
        - text - Python str object - An editted version of the input data `text` 
          with all urls in text replacing with str 'urlplaceholder'.
    """
    
    # get list of all urls using regex
    detected_urls = re.findall(url_regex, text)
    
    # replace each url in text string with placeholder
    for url in detected_urls:
        text = text.replace(url, 'urlplaceholder')
        
    return text

def tokenize(text):
    """
      Takes a Python string object and outputs list of processed words 
       of the text.
      INPUT:
        - text - Python str object - A raw text data
      OUTPUT:
        - stem_words - Python list object - list of processed words using the input `text`.
     """
    #function with raw text data    
    text = replace_urls(text)
    # Removes punctuations and covert to lower case 
    #text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9]", " ", text).lower() 
    text = word_tokenize(text) 
    # Remove stop words
    text = [w for w in text if w not in stopwords.words("english")]
    # Lemmatize verbs by specifying pos
    text = [WordNetLemmatizer().lemmatize(w) for w in text]
    
    return text

### 3. Build a machine learning pipeline
This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.

pipeline = Pipeline([
    ('vect', CountVectorizer(tokenizer=tokenize)),
    ('tfidf', TfidfTransformer()),
    ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs=-1)))
])

pipeline.get_params()

### 4. Train pipeline
- Split data into train and test sets
- Train pipeline

X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=49)
X_train.count()

vect = CountVectorizer(tokenizer=tokenize)
tfidf = TfidfTransformer() 
clf = RandomForestClassifier()  # MultinomialNB()

y_train.isnull().sum()

pipeline.fit(X_train, y_train)
#tfidf.fit_transform(vect.fit_transform(X_test))
pipeline = Pipeline([
         ('vect', CountVectorizer(tokenizer=tokenize)),
         ('tfidf', TfidfTransformer()),
         ('clf', RandomForestClassifier())])

# train classifier
pipeline.fit(X_train, y_train)

# predict on test data
y_pred = pipeline.predict(X_test)

y_pred = pipeline.predict(X_test)

y_pred.shape,y_test.shape,len(list(Y.columns))

### 5. Test your model
Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each.

accuracy = (y_pred == y_test).mean()

print("Accuracy:", accuracy)

print(classification_report(y_test,y_pred,target_names = df.columns[3:]))

### 6. Improve your model
Use grid search to find better parameters. 

pipeline.get_params().keys()



parameters = {
    'clf__n_estimators': [80,90, 100],
     'clf__max_features': [0.65, 0.75,  0.8]
    }
cv = GridSearchCV(
    pipeline, 
    param_grid = parameters,
    cv=3,
    scoring='accuracy'
    )

cv.fit(X_train, y_train)

model = GridSearchCV(pipeline, param_grid=parameters)

model.fit(X_train, y_train)

